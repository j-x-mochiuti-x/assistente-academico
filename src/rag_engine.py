"""
Motor RAG (Retrieval-Augmented Generation).
Respons√°vel por:
1. Criar banco vetorial (embeddings)
2. Buscar documentos relevantes
3. Gerar respostas usando LLM + contexto
"""

import os
from typing import List, Dict, Any, Optional
from pathlib import Path

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableParallel, RunnablePassthrough

from config import EMBEDDING_CONFIG, LLM_CONFIG, RETRIEVAL_CONFIG, CHROMA_DIR

class RAGEngine:
    """
    Motor RAG completo para an√°lise de papers acad√™micos.
    """
    
    def __init__(
        self, 
        embedding_model: str = None,
        llm_model: str = None,
        persist_directory: str = None,
        collection_name: str = "academic_papers"
    ):
        # Configura√ß√µes
        self.embedding_model_name = embedding_model or EMBEDDING_CONFIG["model_name"]
        self.llm_model_name = llm_model or LLM_CONFIG["model"]
        self.persist_dir = persist_directory or str(CHROMA_DIR)
        self.collection_name = collection_name
        
        # Inicializa componentes (lazy loading - s√≥ cria quando necess√°rio)
        self._embeddings = None
        self._llm = None
        self._vectorstore = None
        self._retriever = None
        
        print(f"‚úÖ RAGEngine inicializado")
        print(f"   üì¶ Embedding: {self.embedding_model_name}")
        print(f"   ü§ñ LLM: {self.llm_model_name}")
    
    @property
    def embeddings(self):
        """
        Lazy loading: s√≥ carrega embeddings quando necess√°rio.
        Embeddings s√£o modelos pesados (80MB-2GB), economiza mem√≥ria.
        """
        if self._embeddings is None:
            print(f"‚è≥ Carregando modelo de embeddings: {self.embedding_model_name}...")
            
            # Desativa paralelismo do tokenizador (evita warnings)
            os.environ["TOKENIZERS_PARALLELISM"] = "false"
            
            self._embeddings = HuggingFaceEmbeddings(
                model_name=self.embedding_model_name,
                model_kwargs={'device': 'cpu'},  # Use 'cuda' se tiver GPU
                encode_kwargs={'normalize_embeddings': True}  # Melhora similaridade coseno
            )
            
            print("‚úÖ Embeddings carregados")
        
        return self._embeddings
    
    @property
    def llm(self):
        """
        Lazy loading: s√≥ inicializa LLM quando necess√°rio.
        """
        if self._llm is None:
            if not os.getenv("GROQ_API_KEY"):
                raise ValueError("GROQ_API_KEY n√£o configurada no ambiente")
            
            self._llm = ChatGroq(
                model=self.llm_model_name,
                temperature=LLM_CONFIG["temperature"],
                max_tokens=LLM_CONFIG["max_tokens"]
            )
            
            print(f"‚úÖ LLM inicializado: {self.llm_model_name}")
        
        return self._llm
    
    def create_vectorstore(
        self, 
        documents: List[Document],
        collection_name: str = None
    ) -> Chroma:
        """
        Cria um banco vetorial a partir de documentos.
        
        Este √© o processo de INDEXA√á√ÉO:
        1. Pega cada chunk de texto
        2. Converte em vetor num√©rico (embedding)
        3. Salva no ChromaDB com metadados
        
        Args:
            documents: Lista de chunks processados
            collection_name: Nome da cole√ß√£o no ChromaDB
            
        Returns:
            Inst√¢ncia do ChromaDB
        """
        if not documents:
            raise ValueError("Lista de documentos vazia")
        
        coll_name = collection_name or self.collection_name
        
        print(f"‚è≥ Criando banco vetorial com {len(documents)} chunks...")
        print(f"   üìÅ Salvando em: {self.persist_dir}")
        print(f"   üìÇ Collection: {coll_name}")
        
        # Cria o banco vetorial
        # Isso vai:
        # 1. Gerar embeddings para cada chunk (pode demorar!)
        # 2. Salvar no disco (persist_directory)
        vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=self.embeddings,
            persist_directory=self.persist_dir,
            collection_name=coll_name
        )
        
        print(f"‚úÖ Banco vetorial criado: {vectorstore._collection.count()} vetores")
        
        self._vectorstore = vectorstore
        return vectorstore
    
    def load_vectorstore(self, collection_name: str = None) -> Chroma:
        """
        Carrega um banco vetorial existente do disco.
        
        Args:
            collection_name: Nome da cole√ß√£o
            
        Returns:
            Inst√¢ncia do ChromaDB
        """
        if not Path(self.persist_dir).exists():
            raise FileNotFoundError(f"Banco vetorial n√£o encontrado em: {self.persist_dir}")
        
        coll_name = collection_name or self.collection_name
        print(f"‚è≥ Carregando banco vetorial de: {self.persist_dir}")
        print(f"   üìÇ Collection: {coll_name}")
        
        vectorstore = Chroma(
            persist_directory=self.persist_dir,
            embedding_function=self.embeddings,
            collection_name=coll_name   
        )
        
        print(f"‚úÖ Banco carregado: {vectorstore._collection.count()} vetores")
        
        self._vectorstore = vectorstore
        return vectorstore
    
    def create_retriever(self, k: int = None, search_type: str = "similarity"):
        """
        Cria um recuperador (retriever) a partir do vectorstore.
        
        O retriever √© o componente que busca os chunks mais relevantes.
        
        Args:
            k: N√∫mero de chunks a retornar (padr√£o: config.py)
            search_type: Tipo de busca ("similarity" ou "mmr")
                - similarity: Busca por similaridade coseno simples
                - mmr: Maximum Marginal Relevance (evita redund√¢ncia)
        
        Returns:
            Retriever configurado
        """
        if self._vectorstore is None:
            raise ValueError("Vectorstore n√£o inicializado. Chame create_vectorstore() primeiro.")
        
        k = k or RETRIEVAL_CONFIG["k"]
        
        print(f"‚è≥ Criando retriever (k={k}, tipo={search_type})...")
        
        self._retriever = self._vectorstore.as_retriever(
            search_type=search_type,
            search_kwargs={"k": k}
        )
        
        print("‚úÖ Retriever criado")
        return self._retriever
    
    def format_documents(self, docs: List[Document]) -> str:
        """
        Formata documentos recuperados para inclus√£o no prompt.
        
        Cada chunk vira:
        [arquivo.pdf - p.3 - chunk 5]
        "Conte√∫do do chunk aqui..."
        
        Args:
            docs: Lista de documentos recuperados
            
        Returns:
            String formatada para o prompt
        """
        formatted = []
        
        for i, doc in enumerate(docs, 1):
            meta = doc.metadata
            source = meta.get("source_file", "desconhecido")
            page = meta.get("page", "?")
            chunk_idx = meta.get("chunk_index", "?")
            
            # Trunca conte√∫do muito longo (opcional)
            content = doc.page_content[:1000]
            if len(doc.page_content) > 1000:
                content += "..."
            
            formatted.append(
                f"**[{source} - p.{page} - chunk {chunk_idx}]**\n{content}"
            )
        
        return "\n\n---\n\n".join(formatted)
    
    def create_rag_chain(self):
        """
        Cria o pipeline RAG completo.
        
        Pipeline: Pergunta ‚Üí Busca no Vectorstore ‚Üí Formata Contexto ‚Üí LLM ‚Üí Resposta
        
        Returns:
            Chain execut√°vel
        """
        if self._retriever is None:
            raise ValueError("Retriever n√£o criado. Chame create_retriever() primeiro.")
        
        # Define o prompt do sistema
        system_prompt = """Voc√™ √© um assistente acad√™mico especializado em an√°lise de papers cient√≠ficos.

Sua tarefa √© responder perguntas baseando-se ESTRITAMENTE no contexto fornecido dos papers.

Diretrizes:
1. **Cite as fontes**: Sempre mencione de qual paper veio cada informa√ß√£o (ex: "Segundo Silva et al. (2024)...")
2. **Seja preciso**: Se a resposta n√£o estiver no contexto, diga "N√£o encontrei essa informa√ß√£o nos papers fornecidos"
3. **Estruture bem**: Use se√ß√µes como "Resumo", "Detalhes", "Limita√ß√µes" quando apropriado
4. **Compare quando pedido**: Se perguntarem sobre diferen√ßas entre estudos, fa√ßa compara√ß√£o direta
5. **Linguagem acad√™mica**: Use terminologia t√©cnica apropriada, mas seja claro

Contexto dos Papers:
{context}"""

        # Cria o template de prompt
        prompt = ChatPromptTemplate.from_messages([
            SystemMessage(content=system_prompt),
            ("human", "{question}")
        ])
        
        # Cria o pipeline RAG
        # RunnableParallel executa em paralelo:
        # - context: busca + formata√ß√£o dos documentos
        # - question: apenas passa a pergunta adiante
        rag_chain = (
            RunnableParallel(
                context=self._retriever | self.format_documents,
                question=RunnablePassthrough()
            )
            | prompt
            | self.llm
            | StrOutputParser()
        )
        
        print("‚úÖ Pipeline RAG criado")
        return rag_chain
    
    def query(
    self,
    question: str, 
    return_sources: bool = True
) -> Dict[str, Any]:
        if self._retriever is None:
            raise ValueError("Sistema RAG n√£o inicializado completamente")
    
        print(f"‚è≥ Processando pergunta: {question[:50]}...")
    
        # 1. Busca documentos relevantes
        retrieved_docs = self._retriever.invoke(question)
        
        print(f"üêõ DEBUG - Documentos recuperados: {len(retrieved_docs)}")
        
        # 2. Formata o contexto
        context = self.format_documents(retrieved_docs)
        
        print(f"üêõ DEBUG - Tamanho do contexto: {len(context)} chars")
        
        # 3. Define o prompt (inline, n√£o separado)
        system_prompt = """Voc√™ √© um assistente acad√™mico especializado em an√°lise de papers cient√≠ficos.

    Sua tarefa √© responder perguntas baseando-se ESTRITAMENTE no contexto fornecido dos papers.

    Diretrizes:
    1. **Cite as fontes**: Sempre mencione de qual paper veio cada informa√ß√£o (ex: "Segundo o documento...")
    2. **Seja preciso**: Se a resposta n√£o estiver no contexto, diga "N√£o encontrei essa informa√ß√£o nos papers fornecidos"
    3. **Estruture bem**: Use se√ß√µes como "Resumo", "Detalhes" quando apropriado
    4. **Linguagem acad√™mica**: Use terminologia t√©cnica apropriada, mas seja claro

    Contexto dos Papers:
    {context}

    Pergunta: {question}"""

        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt)
        ])
        
        # 4. Cria chain simples
        chain = prompt | self.llm | StrOutputParser()
        
        # 5. Executa chain com contexto e pergunta
        answer = chain.invoke({
            "context": context,
            "question": question
        })
        
        # 6. Prepara resultado
        result = {
            "answer": answer,
            "metadata": {
                "chunks_retrieved": len(retrieved_docs),
                "model": self.llm_model_name,
                "embedding_model": self.embedding_model_name
            }
        }
        
        if return_sources:
            result["sources"] = retrieved_docs
        
        print("‚úÖ Resposta gerada")
        return result

    def query_with_filters(
        self,
        question: str,
        author: str = None,
        year: int = None,
        return_sources: bool = True
    ) -> Dict[str, Any]:
        """
        Faz uma pergunta ao sistema RAG com filtros de metadados.
        
        Args:
            question: Pergunta do usu√°rio
            author: Filtrar por autor (opcional)
            year: Filtrar por ano (opcional)
            return_sources: Se True, retorna os chunks usados
            
        Returns:
            Dicion√°rio com resposta e metadados
        """
        if self._vectorstore is None:
            raise ValueError("Vectorstore n√£o inicializado")
        
        print(f"‚è≥ Processando pergunta com filtros...")
        if author:
            print(f"   üë§ Filtro de autor: {author}")
        if year:
            print(f"   üìÖ Filtro de ano: {year}")
        
        # Constr√≥i filtro para ChromaDB
        filter_dict = {}
        if author:
            filter_dict["author"] = author
        if year:
            filter_dict["year"] = year
        
        # Busca com filtros
        if filter_dict:
            # Usa similarity_search com filtro
            retrieved_docs = self._vectorstore.similarity_search(
                query=question,
                k=RETRIEVAL_CONFIG["k"],
                filter=filter_dict
            )
        else:
            # Busca normal sem filtros
            retrieved_docs = self._retriever.invoke(question)
        
        print(f"üêõ DEBUG - Documentos recuperados (com filtros): {len(retrieved_docs)}")
        
        if not retrieved_docs:
            return {
                "answer": f"N√£o encontrei documentos correspondentes aos filtros especificados (autor: {author}, ano: {year}).",
                "sources": [],
                "metadata": {
                    "chunks_retrieved": 0,
                    "filters_applied": filter_dict
                }
            }
        
        # Formata contexto
        context = self.format_documents(retrieved_docs)
        
        # Define prompt
        system_prompt = """Voc√™ √© um assistente acad√™mico especializado em an√°lise de papers cient√≠ficos.

    Sua tarefa √© responder perguntas baseando-se ESTRITAMENTE no contexto fornecido dos papers.

    Diretrizes:
    1. **Cite as fontes**: Sempre mencione de qual paper veio cada informa√ß√£o
    2. **Seja preciso**: Se a resposta n√£o estiver no contexto, diga claramente
    3. **Estruture bem**: Use se√ß√µes quando apropriado
    4. **Linguagem acad√™mica**: Use terminologia t√©cnica, mas seja claro

    Contexto dos Papers:
    {context}

    Pergunta: {question}"""

        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt)
        ])
        
        # Cria chain
        chain = prompt | self.llm | StrOutputParser()
        
        # Executa
        answer = chain.invoke({
            "context": context,
            "question": question
        })
        
        # Resultado
        result = {
            "answer": answer,
            "metadata": {
                "chunks_retrieved": len(retrieved_docs),
                "filters_applied": filter_dict,
                "model": self.llm_model_name,
                "embedding_model": self.embedding_model_name
            }
        }
        
        if return_sources:
            result["sources"] = retrieved_docs
        
        print("‚úÖ Resposta gerada")
        return result
