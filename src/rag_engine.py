"""
Motor RAG (Retrieval-Augmented Generation).
ResponsÃ¡vel por:
1. Criar banco vetorial (embeddings)
2. Buscar documentos relevantes
3. Gerar respostas usando LLM + contexto
"""

import os
from typing import List, Dict, Any, Optional
from pathlib import Path

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import SystemMessage
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableParallel, RunnablePassthrough

from config import EMBEDDING_CONFIG, LLM_CONFIG, RETRIEVAL_CONFIG, CHROMA_DIR


class RAGEngine:
    """
    Motor RAG completo para anÃ¡lise de papers acadÃªmicos.
    """
    
    def __init__(
        self, 
        embedding_model: str = None,
        llm_model: str = None,
        persist_directory: str = None
    ):
        """
        Inicializa o motor RAG.
        
        Args:
            embedding_model: Nome do modelo de embedding (padrÃ£o: config.py)
            llm_model: Nome do modelo LLM (padrÃ£o: config.py)
            persist_directory: DiretÃ³rio para salvar vetores (padrÃ£o: config.py)
        """
        # ConfiguraÃ§Ãµes
        self.embedding_model_name = embedding_model or EMBEDDING_CONFIG["model_name"]
        self.llm_model_name = llm_model or LLM_CONFIG["model"]
        self.persist_dir = persist_directory or str(CHROMA_DIR)
        
        # Inicializa componentes (lazy loading - sÃ³ cria quando necessÃ¡rio)
        self._embeddings = None
        self._llm = None
        self._vectorstore = None
        self._retriever = None
        
        print(f"âœ… RAGEngine inicializado")
        print(f"   ðŸ“¦ Embedding: {self.embedding_model_name}")
        print(f"   ðŸ¤– LLM: {self.llm_model_name}")
    
    @property
    def embeddings(self):
        """
        Lazy loading: sÃ³ carrega embeddings quando necessÃ¡rio.
        Embeddings sÃ£o modelos pesados (80MB-2GB), economiza memÃ³ria.
        """
        if self._embeddings is None:
            print(f"â³ Carregando modelo de embeddings: {self.embedding_model_name}...")
            
            # Desativa paralelismo do tokenizador (evita warnings)
            os.environ["TOKENIZERS_PARALLELISM"] = "false"
            
            self._embeddings = HuggingFaceEmbeddings(
                model_name=self.embedding_model_name,
                model_kwargs={'device': 'cpu'},  # Use 'cuda' se tiver GPU
                encode_kwargs={'normalize_embeddings': True}  # Melhora similaridade coseno
            )
            
            print("âœ… Embeddings carregados")
        
        return self._embeddings
    
    @property
    def llm(self):
        """
        Lazy loading: sÃ³ inicializa LLM quando necessÃ¡rio.
        """
        if self._llm is None:
            if not os.getenv("GROQ_API_KEY"):
                raise ValueError("GROQ_API_KEY nÃ£o configurada no ambiente")
            
            self._llm = ChatGroq(
                model=self.llm_model_name,
                temperature=LLM_CONFIG["temperature"],
                max_tokens=LLM_CONFIG["max_tokens"]
            )
            
            print(f"âœ… LLM inicializado: {self.llm_model_name}")
        
        return self._llm
    
    def create_vectorstore(
        self, 
        documents: List[Document],
        collection_name: str = "academic_papers"
    ) -> Chroma:
        """
        Cria um banco vetorial a partir de documentos.
        
        Este Ã© o processo de INDEXAÃ‡ÃƒO:
        1. Pega cada chunk de texto
        2. Converte em vetor numÃ©rico (embedding)
        3. Salva no ChromaDB com metadados
        
        Args:
            documents: Lista de chunks processados
            collection_name: Nome da coleÃ§Ã£o no ChromaDB
            
        Returns:
            InstÃ¢ncia do ChromaDB
        """
        if not documents:
            raise ValueError("Lista de documentos vazia")
        
        print(f"â³ Criando banco vetorial com {len(documents)} chunks...")
        print(f"   ðŸ“ Salvando em: {self.persist_dir}")
        
        # Cria o banco vetorial
        # Isso vai:
        # 1. Gerar embeddings para cada chunk (pode demorar!)
        # 2. Salvar no disco (persist_directory)
        vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=self.embeddings,
            persist_directory=self.persist_dir,
            collection_name=collection_name
        )
        
        print(f"âœ… Banco vetorial criado: {vectorstore._collection.count()} vetores")
        
        self._vectorstore = vectorstore
        return vectorstore
    
    def load_vectorstore(self, collection_name: str = "academic_papers") -> Chroma:
        """
        Carrega um banco vetorial existente do disco.
        
        Args:
            collection_name: Nome da coleÃ§Ã£o
            
        Returns:
            InstÃ¢ncia do ChromaDB
        """
        if not Path(self.persist_dir).exists():
            raise FileNotFoundError(f"Banco vetorial nÃ£o encontrado em: {self.persist_dir}")
        
        print(f"â³ Carregando banco vetorial de: {self.persist_dir}")
        
        vectorstore = Chroma(
            persist_directory=self.persist_dir,
            embedding_function=self.embeddings,
            collection_name=collection_name
        )
        
        print(f"âœ… Banco carregado: {vectorstore._collection.count()} vetores")
        
        self._vectorstore = vectorstore
        return vectorstore
    
    def create_retriever(self, k: int = None, search_type: str = "similarity"):
        """
        Cria um recuperador (retriever) a partir do vectorstore.
        
        O retriever Ã© o componente que busca os chunks mais relevantes.
        
        Args:
            k: NÃºmero de chunks a retornar (padrÃ£o: config.py)
            search_type: Tipo de busca ("similarity" ou "mmr")
                - similarity: Busca por similaridade coseno simples
                - mmr: Maximum Marginal Relevance (evita redundÃ¢ncia)
        
        Returns:
            Retriever configurado
        """
        if self._vectorstore is None:
            raise ValueError("Vectorstore nÃ£o inicializado. Chame create_vectorstore() primeiro.")
        
        k = k or RETRIEVAL_CONFIG["k"]
        
        print(f"â³ Criando retriever (k={k}, tipo={search_type})...")
        
        self._retriever = self._vectorstore.as_retriever(
            search_type=search_type,
            search_kwargs={"k": k}
        )
        
        print("âœ… Retriever criado")
        return self._retriever
    
    def format_documents(self, docs: List[Document]) -> str:
        """
        Formata documentos recuperados para inclusÃ£o no prompt.
        
        Cada chunk vira:
        [arquivo.pdf - p.3 - chunk 5]
        "ConteÃºdo do chunk aqui..."
        
        Args:
            docs: Lista de documentos recuperados
            
        Returns:
            String formatada para o prompt
        """
        formatted = []
        
        for i, doc in enumerate(docs, 1):
            meta = doc.metadata
            source = meta.get("source_file", "desconhecido")
            page = meta.get("page", "?")
            chunk_idx = meta.get("chunk_index", "?")
            
            # Trunca conteÃºdo muito longo (opcional)
            content = doc.page_content[:1000]
            if len(doc.page_content) > 1000:
                content += "..."
            
            formatted.append(
                f"**[{source} - p.{page} - chunk {chunk_idx}]**\n{content}"
            )
        
        return "\n\n---\n\n".join(formatted)
    
    def create_rag_chain(self):
        """
        Cria o pipeline RAG completo.
        
        Pipeline: Pergunta â†’ Busca no Vectorstore â†’ Formata Contexto â†’ LLM â†’ Resposta
        
        Returns:
            Chain executÃ¡vel
        """
        if self._retriever is None:
            raise ValueError("Retriever nÃ£o criado. Chame create_retriever() primeiro.")
        
        # Define o prompt do sistema
        system_prompt = """VocÃª Ã© um assistente acadÃªmico especializado em anÃ¡lise de papers cientÃ­ficos.

Sua tarefa Ã© responder perguntas baseando-se ESTRITAMENTE no contexto fornecido dos papers.

Diretrizes:
1. **Cite as fontes**: Sempre mencione de qual paper veio cada informaÃ§Ã£o (ex: "Segundo Silva et al. (2024)...")
2. **Seja preciso**: Se a resposta nÃ£o estiver no contexto, diga "NÃ£o encontrei essa informaÃ§Ã£o nos papers fornecidos"
3. **Estruture bem**: Use seÃ§Ãµes como "Resumo", "Detalhes", "LimitaÃ§Ãµes" quando apropriado
4. **Compare quando pedido**: Se perguntarem sobre diferenÃ§as entre estudos, faÃ§a comparaÃ§Ã£o direta
5. **Linguagem acadÃªmica**: Use terminologia tÃ©cnica apropriada, mas seja claro

Contexto dos Papers:
{context}"""

        # Cria o template de prompt
        prompt = ChatPromptTemplate.from_messages([
            SystemMessage(content=system_prompt),
            ("human", "{question}")
        ])
        
        # Cria o pipeline RAG
        # RunnableParallel executa em paralelo:
        # - context: busca + formataÃ§Ã£o dos documentos
        # - question: apenas passa a pergunta adiante
        rag_chain = (
            RunnableParallel(
                context=self._retriever | self.format_documents,
                question=RunnablePassthrough()
            )
            | prompt
            | self.llm
            | StrOutputParser()
        )
        
        print("âœ… Pipeline RAG criado")
        return rag_chain
    
    def query(
        self, 
        question: str, 
        return_sources: bool = True
    ) -> Dict[str, Any]:
        """
        Faz uma pergunta ao sistema RAG.
        
        Este Ã© o mÃ©todo principal que vocÃª vai usar!
        
        Args:
            question: Pergunta do usuÃ¡rio
            return_sources: Se True, retorna os chunks usados
            
        Returns:
            DicionÃ¡rio com:
            - answer: Resposta gerada
            - sources: Chunks recuperados (se return_sources=True)
            - metadata: InformaÃ§Ãµes sobre a busca
        """
        if self._retriever is None:
            raise ValueError("Sistema RAG nÃ£o inicializado completamente")
        
        print(f"â³ Processando pergunta: {question[:50]}...")
        
        # 1. Busca documentos relevantes
        retrieved_docs = self._retriever.get_relevant_documents(question)
        
        # 2. Cria a chain
        rag_chain = self.create_rag_chain()
        
        # 3. Gera resposta
        answer = rag_chain.invoke(question)
        
        # 4. Prepara resultado
        result = {
            "answer": answer,
            "metadata": {
                "chunks_retrieved": len(retrieved_docs),
                "model": self.llm_model_name,
                "embedding_model": self.embedding_model_name
            }
        }
        
        if return_sources:
            result["sources"] = retrieved_docs
        
        print("âœ… Resposta gerada")
        return result


# FunÃ§Ã£o auxiliar para uso rÃ¡pido
def create_rag_system(documents: List[Document]) -> RAGEngine:
    """
    Cria um sistema RAG completo em um comando.
    
    Usage:
        rag = create_rag_system(processed_chunks)
        result = rag.query("Qual a metodologia usada?")
        print(result["answer"])
    """
    engine = RAGEngine()
    engine.create_vectorstore(documents)
    engine.create_retriever()
    return engine