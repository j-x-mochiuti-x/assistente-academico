"""
Configura√ß√µes centralizadas do projeto.
Facilita manuten√ß√£o e evita valores hardcoded.
"""
import os
from pathlib import Path

# Diret√≥rios do projeto
BASE_DIR = Path(__file__).parent
DATA_DIR = BASE_DIR / "data"
CHROMA_DIR = BASE_DIR / "chroma_db"

# Criar diret√≥rios se n√£o existirem
DATA_DIR.mkdir(exist_ok=True)
CHROMA_DIR.mkdir(exist_ok=True)

# Configura√ß√µes do Modelo LLM
LLM_CONFIG = {
    "model": "llama-3.3-70b-versatile",  # Modelo mais capaz para an√°lise acad√™mica
    "temperature": 0.3,  # Um pouco mais criativo que 0.2 para s√≠ntese
    "max_tokens": 2048   # Mais tokens para respostas elaboradas
}

# Configura√ß√µes de Embeddings
EMBEDDING_CONFIG = {
    "model_name": "sentence-transformers/all-MiniLM-L6-v2",  # Modelo leve e eficiente
    # Alternativa mais precisa (mas mais lenta): "sentence-transformers/msmarco-bert-base-dot-v5"
}

# Configura√ß√µes de Chunking (divis√£o de texto)
CHUNK_CONFIG = {
    "chunk_size": 1200,      # Chunks maiores para contexto acad√™mico
    "chunk_overlap": 200,    # Sobreposi√ß√£o para n√£o perder contexto
}

# Configura√ß√µes de Retrieval (busca)
RETRIEVAL_CONFIG = {
    "k": 5,  # N√∫mero de chunks relevantes a buscar
}

# Configura√ß√µes da Interface
UI_CONFIG = {
    "page_title": "Assistente Acad√™mico",
    "page_icon": "üéì",
    "layout": "wide"
}

# Configura√ß√µes do Sistema RAG
RAG_SYSTEM_PROMPT = """Voc√™ √© um assistente acad√™mico especializado em an√°lise de papers cient√≠ficos.

Sua tarefa √© responder perguntas baseando-se ESTRITAMENTE no contexto fornecido dos papers.

Diretrizes:
1. **Cite as fontes**: Sempre mencione de qual paper veio cada informa√ß√£o
2. **Seja preciso**: Se a resposta n√£o estiver no contexto, diga claramente
3. **Estruture bem**: Use se√ß√µes quando apropriado
4. **Compare quando pedido**: Fa√ßa compara√ß√µes diretas entre estudos
5. **Linguagem acad√™mica**: Use terminologia t√©cnica, mas seja claro
"""